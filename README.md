# ğŸ•¸ï¸ PubMed Crawler ğŸ§¬

## ğŸ“„ DescripciÃ³n
**PubMed Crawler** es un crawler web desarrollado con **Scrapy**, diseÃ±ado especÃ­ficamente para rastrear y extraer informaciÃ³n del sitio web de **PubMed** (https://pubmed.ncbi.nlm.nih.gov). Este proyecto tiene como objetivo recopilar artÃ­culos cientÃ­ficos que cumplan con criterios especÃ­ficos, con el fin de ayudar a las personas a tener una bÃºsqueda mÃ¡s rÃ¡pida y especializadea, almacenando los datos en una base de datos **SQLite** para su posterior anÃ¡lisis.

## âœ¨ CaracterÃ­sticas
- ğŸŒ **Dominio especÃ­fico**: Limita el rastreo al dominio de PubMed.
- ğŸ” **Profundidad de rastreo**: Configurable hasta mÃ¡ximo 4 niveles de profundidad.
- ğŸ”— **Inicio personalizable**: Permite al usuario especificar una URL de inicio (si no se especifica, tiene una por **default**).
- ğŸ©º **Filtrado por palabras clave**: Solo extrae pÃ¡ginas que contienen ciertas palabras clave mÃ©dicas y cientÃ­ficas.
- ğŸ“¹ **Tipos de contenido**: Verifica la presencia de imÃ¡genes, videos, PDFs, tablas, figuras y enlaces externos.
- ğŸ•“ **ActualizaciÃ³n reciente**: Rastrear solo pÃ¡ginas actualizadas en los Ãºltimos 180 dÃ­as.
- ğŸ“ **TamaÃ±o de pÃ¡gina**: Descarta pÃ¡ginas que exceden 2 MB.
- ğŸ”– **Etiquetas HTML requeridas**: Solo procesa pÃ¡ginas que contienen ciertas etiquetas HTML.
- ğŸŒ **Idioma**: Filtra pÃ¡ginas para que solo se procesen aquellas en inglÃ©s.
- ğŸ”„ **Enlaces internos**: Sigue Ãºnicamente enlaces internos dentro del dominio permitido.
- ğŸ—ºï¸ **UbicaciÃ³n geogrÃ¡fica**: Limita el rastreo a servidores ubicados en **Estados Unidos**.
- ğŸ“Š **Estructura de URL**: Solo sigue URLs que contienen un patrÃ³n especÃ­fico.
- ğŸ”’ **Evita autenticaciÃ³n**: Descarta pÃ¡ginas que requieren autenticaciÃ³n o parecen ser pÃ¡ginas de inicio de sesiÃ³n.

## ğŸ’» TecnologÃ­as usadas
- **Python 3**
- **Chrome 114**
- **Scrapy**: Framework para la extracciÃ³n de datos web.
- **Langid**: Biblioteca para la detecciÃ³n de idioma.
- **GeoIP2**: Para determinar la ubicaciÃ³n geogrÃ¡fica de los servidores.
- **SQLite3**: Base de datos ligera para almacenar los datos extraÃ­dos.
- **Dateutil**: Para manejar fechas y tiempos.
- **Otras bibliotecas estÃ¡ndar de Python**: `datetime`, `re`, `urllib`, `socket`, etc.

## âœ… Requisitos del proyecto
El proyecto cumple con los siguientes requisitos:

1. ğŸŒ **Por dominio**: Limita el rastreo al dominio `pubmed.ncbi.nlm.nih.gov`.
2. ğŸ” **Por profundidad**: Establece un lÃ­mite de 4 niveles de profundidad.
3. ğŸ”— **Por URL**: Permite ingresar una URL especÃ­fica para comenzar el rastreo.
4. ğŸ©º **Por palabras clave**: Filtra pÃ¡ginas que contienen ciertas palabras clave definidas en la configuraciÃ³n.
5. ğŸ“¹ **Por tipo de contenido**: Solo procesa pÃ¡ginas que contienen tipos de contenido especÃ­ficos (imÃ¡genes, videos, PDFs, etc.).
6. ğŸ•“ **Por frecuencia de actualizaciÃ³n**: Rastrear solo pÃ¡ginas actualizadas en los Ãºltimos 180 dÃ­as.
7. ğŸ“ **Por tamaÃ±o de pÃ¡gina**: Limita el tamaÃ±o de la pÃ¡gina a 2 MB.
8. ğŸ”– **Por etiquetas HTML**: Solo procesa pÃ¡ginas que contienen ciertas etiquetas HTML (e.g., `<meta name="description">`).
9. ğŸ“… **Por fecha de publicaciÃ³n**: Rastrear solo pÃ¡ginas publicadas despuÃ©s del 1 de enero de 2023.
10. ğŸŒ **Por idioma**: Filtra pÃ¡ginas en inglÃ©s.
11. ğŸ”„ **Por tipo de enlace**: Sigue Ãºnicamente enlaces internos.
12. ğŸ—ºï¸ **Por ubicaciÃ³n geogrÃ¡fica**: Limita el rastreo a servidores en **Estados Unidos**.
13. ğŸ”— **Por estructura de URL**: Sigue solo URLs que contienen el patrÃ³n `.*pubmed.*`.
14. ğŸ”’ **Por restricciones de acceso**: Evita pÃ¡ginas que requieren autenticaciÃ³n.
15. ğŸ•“ **Por fecha de modificaciÃ³n**: Rastrear solo pÃ¡ginas modificadas recientemente (Ãºltimos 180 dÃ­as).
16. ğŸ” **Por patrones de URL**: Sigue URLs que coinciden con un patrÃ³n especÃ­fico.
17. ğŸ§  **Por contexto semÃ¡ntico**: Filtra pÃ¡ginas relacionadas semÃ¡nticamente con temas mÃ©dicos y cientÃ­ficos.
18. âœ… **Por popularidad**: Tiene un contador de popularidad de enlaces.

**Nota**: Los requisitos 'por sitemap' y 'por autoridad del dominio' no se cumplen en la versiÃ³n actual del crawler.

## âš™ï¸ Â¿CÃ³mo funciona?
El crawler utiliza **Scrapy** para rastrear el sitio web de **PubMed** siguiendo reglas y filtros definidos en la configuraciÃ³n (`CONFIG`). A continuaciÃ³n, se detalla su funcionamiento:

1. **InicializaciÃ³n**: 
   - Carga la configuraciÃ³n y establece la URL de inicio.
   - Carga la base de datos **GeoIP2** para verificar ubicaciones geogrÃ¡ficas.

2. **Rastreo**:
   - Sigue enlaces internos dentro del dominio permitido, respetando la profundidad mÃ¡xima y otros lÃ­mites.

3. **Filtrado**:
   - Verifica que la pÃ¡gina no requiera autenticaciÃ³n.
   - Comprueba la ubicaciÃ³n geogrÃ¡fica del servidor.
   - Verifica el tamaÃ±o de la pÃ¡gina.
   - Confirma la presencia de etiquetas HTML requeridas.
   - Verifica que contenga el tipo de contenido requerido.
   - Extrae y verifica la fecha de publicaciÃ³n.
   - Detecta el idioma del contenido.
   - Busca palabras clave en el texto.

4. **ExtracciÃ³n**:
   - Si la pÃ¡gina cumple con todos los criterios, extrae la informaciÃ³n relevante:
     - URL
     - TÃ­tulo
     - Fecha de publicaciÃ³n
     - Autores
     - Palabras clave encontradas
     - Fragmento del contenido
     - Tipo de contenido encontrado
     - Popularidad de enlaces entrantes

5. **Almacenamiento**:
   - Utiliza un pipeline para almacenar los datos extraÃ­dos en una base de datos **SQLite** (`datos_rastreados.db`).

## ğŸš€ Instrucciones de instalaciÃ³n y uso

### Requisitos previos
- **Python 3.13.0**
- **Scrapy**: Se puede instalar usando `pip install scrapy`
- **GeoIP2**: `pip install geoip2`
- **Langid**: `pip install langid`
- **Dateutil**: `pip install python-dateutil`

### ğŸ“¥ Descarga de la Base de Datos GeoLite2
1. Descarga el archivo **GeoLite2-City.mmdb** desde [MaxMind GeoLite2](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data).
2. Coloca el archivo **GeoLite2-City.mmdb** en el directorio raÃ­z del proyecto.

### â–¶ï¸ EjecuciÃ³n del Crawler
1. Clona el repositorio o descarga los archivos del proyecto.

2. Navega al directorio del proyecto:
   cd pubmed_crawler
3. Instala las dependencias
4. Ejecuta el crawler
   scrapy crawl pubmed
   Puedes especificar una URL diferente:
       scrapy crawl pubmed -a start_url="https://pubmed.ncbi.nlm.nih.gov/specific_page"
   O un output en formato .json para cuestiÃ³n de pruebas:
       scrapy crawl pubmed -o output.json
### ğŸ“Š Acceso a los datos extraÃ­dos
Los datos se almacenan en la base de datos SQLite datos_rastreados.db.
Puedes acceder a los datos utilizando herramientas como sqlite3 en la lÃ­nea de comandos o herramientas grÃ¡ficas como "DB Browser for SQLite": 
  sqlite3 datos_rastreados.db
O en la consola de SQLite:
  SELECT * FROM articulos;
### ğŸ›‘ Consideraciones Ã©ticas y legales
* Respeto a las oolÃ­ticas del sitio: El crawler respeta las directivas de robots.txt y limita la carga en el servidor mediante retrasos y lÃ­mites en las solicitudes.
* Datos pÃºblicos: El crawler solo extrae informaciÃ³n disponible pÃºblicamente y no accede a datos privados o protegidos.

### Datos personales
* Nombre completo: HernÃ¡ndez Cortez Kevin Uriel.
* CÃ³digo: 217734547.
* Materia: ProgramaciÃ³n para internet - D17.
* Profesor: Guzman Montes Carlos Alberto.
* Fecha 29/11/24

Disfruta rastreando PubMed de manera eficiente y Ã©tica con PubMed Crawler! ğŸ•¸ï¸âœ¨
